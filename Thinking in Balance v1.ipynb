{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84e58ec0-1dd3-4395-886a-074b16bc8cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cognitive Load Balancing Simulation ===\n",
      "Users: ['Alice:A', 'Bob:B', 'Cara:C', 'Dan:D']\n",
      "Generating 20 tasksâ€¦\n",
      "\n",
      "âœ“ Assigned Task#0 (analytical, L=2.14, eff=1.72) to Alice. Load=1.72\n",
      "âœ“ Assigned Task#1 (sequential, L=4.84, eff=3.87) to Bob. Load=3.87\n",
      "âœ“ Assigned Task#2 (interpersonal, L=3.69, eff=2.95) to Cara. Load=2.95\n",
      "âœ“ Assigned Task#3 (analytical, L=2.81, eff=2.81) to Dan. Load=2.81\n",
      "âœ“ Assigned Task#4 (imaginative, L=3.44, eff=3.44) to Alice. Load=5.15\n",
      "ðŸ Alice completed Task#0 [analytical] in 1.8s with ERROR. New Load=3.44\n",
      "ðŸ Bob completed Task#1 [sequential] in 3.5s with SUCCESS. New Load=0.00\n",
      "ðŸ Cara completed Task#2 [interpersonal] in 3.2s with SUCCESS. New Load=0.00\n",
      "ðŸ Dan completed Task#3 [analytical] in 3.3s with SUCCESS. New Load=0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 308\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tasks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tasksâ€¦\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    307\u001b[0m sim \u001b[38;5;241m=\u001b[39m Simulation(users, tasks)\n\u001b[0;32m--> 308\u001b[0m \u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43massign_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mASSIGN_BATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 237\u001b[0m, in \u001b[0;36mSimulation.run\u001b[0;34m(self, assign_batch)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m pending \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m<\u001b[39m assign_batch:\n\u001b[1;32m    236\u001b[0m     batch\u001b[38;5;241m.\u001b[39mappend(pending\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m--> 237\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# pause for half a second To be able to see \u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Assign current batch\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# \"Thinking in balance: Framework for cognitive load distribution in human-AI collaborative teams\" \n",
    "# \n",
    "# Cognitive Load Balancing Simulation (backend only)\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import time\n",
    "import itertools\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "# ------------------------------- Configuration -------------------------------\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "MAX_LOAD = 10.0                 # Soft cap for per-user cognitive load\n",
    "TASK_COUNT = 20                 # How many tasks to simulate\n",
    "ASSIGN_BATCH_SIZE = 5           # Assign tasks in batches, then rebalance\n",
    "COMPLETE_PER_USER_PER_ROUND = 1 # How many tasks each user completes per round\n",
    "PRINT_EVENTS = True             # Toggle logging\n",
    "\n",
    "# Task types and their labels (BDI styles)\n",
    "TASK_TYPES = [\"analytical\", \"sequential\", \"interpersonal\", \"imaginative\"]\n",
    "\n",
    "# Base duration multiplier (for proxying latency); higher => tasks take longer\n",
    "BASE_TIME_PER_LOAD = 1.0\n",
    "\n",
    "# Error probabilities by current load band (behavioral proxy)\n",
    "ERROR_PROB_BANDS = [\n",
    "    (8.0, 0.30),  # load > 8.0 => 30% error chance\n",
    "    (5.0, 0.10),  # load > 5.0 => 10% error chance\n",
    "    (0.0, 0.02),  # otherwise => 2% error chance\n",
    "]\n",
    "\n",
    "# ------------------------------- Data Models ---------------------------------\n",
    "\n",
    "@dataclass\n",
    "class Task:\n",
    "    id: int\n",
    "    task_type: str                  # \"analytical\" | \"sequential\" | \"interpersonal\" | \"imaginative\"\n",
    "    intrinsic_load: float           # baseline cognitive effort\n",
    "    assigned_to: Optional[\"User\"] = None\n",
    "    status: str = \"pending\"         # \"pending\" | \"assigned\" | \"completed\"\n",
    "    # for simple switching metric (how many times we move/reassign the task)\n",
    "    reassignments: int = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Task#{self.id}({self.task_type}, L={self.intrinsic_load:.2f}, {self.status})\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class User:\n",
    "    name: str\n",
    "    style: str                      # 'A' | 'B' | 'C' | 'D'\n",
    "    current_load: float = 0.0\n",
    "    # Skill factor lowers effective load for preferred task types (good fit < 1.0)\n",
    "    skill_factor: Dict[str, float] = field(default_factory=dict)\n",
    "    # Active tasks assigned (not completed yet)\n",
    "    active_tasks: List[Task] = field(default_factory=list)\n",
    "    # For simple \"task switching\" proxy: how many distinct task types in a row\n",
    "    recent_types: List[str] = field(default_factory=list)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Default skill mapping if not provided\n",
    "        if not self.skill_factor:\n",
    "            self.skill_factor = {\n",
    "                \"analytical\":   0.8 if self.style == 'A' else 1.0,\n",
    "                \"sequential\":   0.8 if self.style == 'B' else 1.0,\n",
    "                \"interpersonal\":0.8 if self.style == 'C' else 1.0,\n",
    "                \"imaginative\":     0.8 if self.style == 'D' else 1.0,\n",
    "            }\n",
    "\n",
    "    def effective_load(self, task: Task) -> float:\n",
    "        return task.intrinsic_load * self.skill_factor.get(task.task_type, 1.0)\n",
    "\n",
    "# ------------------------------- Agent Layer ---------------------------------\n",
    "\n",
    "class ManagerAI:\n",
    "    \"\"\"Per-user manager that reports local state and simple heuristics.\"\"\"\n",
    "    def __init__(self, user: User):\n",
    "        self.user = user\n",
    "\n",
    "    def report_load(self) -> float:\n",
    "        return self.user.current_load\n",
    "\n",
    "    def can_take_task(self, task: Task) -> bool:\n",
    "        return (self.user.current_load + self.user.effective_load(task)) <= MAX_LOAD\n",
    "\n",
    "    def accept_task(self, task: Task):\n",
    "        self.user.active_tasks.append(task)\n",
    "        self.user.current_load += self.user.effective_load(task)\n",
    "        # Update simple \"switching\" proxy: track a short window of recent types\n",
    "        self.user.recent_types.append(task.task_type)\n",
    "        if len(self.user.recent_types) > 5:\n",
    "            self.user.recent_types.pop(0)\n",
    "\n",
    "    def release_task(self, task: Task):\n",
    "        if task in self.user.active_tasks:\n",
    "            self.user.active_tasks.remove(task)\n",
    "            self.user.current_load -= self.user.effective_load(task)\n",
    "            if self.user.current_load < 0:\n",
    "                self.user.current_load = 0.0\n",
    "\n",
    "class CoordinatorAI:\n",
    "    \"\"\"Global coordinator that assigns, monitors, and rebalances tasks.\"\"\"\n",
    "    def __init__(self, users: List[User]):\n",
    "        self.users = users\n",
    "        self.managers: Dict[str, ManagerAI] = {u.name: ManagerAI(u) for u in users}\n",
    "\n",
    "    # -------------------------- Assignment & Scoring --------------------------\n",
    "\n",
    "    def find_best_user_for(self, task: Task) -> Optional[User]:\n",
    "        \"\"\"Choose the user with the lowest projected load after assignment,\n",
    "        favoring better style-task fit (lower effective load).\"\"\"\n",
    "        best_user = None\n",
    "        best_score = float(\"inf\")\n",
    "        for u in self.users:\n",
    "            projected = u.current_load + u.effective_load(task)\n",
    "            if projected <= MAX_LOAD:\n",
    "                # score = projected load; can add tie-break on skill factor\n",
    "                if projected < best_score:\n",
    "                    best_score = projected\n",
    "                    best_user = u\n",
    "        return best_user\n",
    "\n",
    "    def assign_task(self, task: Task, user: User):\n",
    "        if task.assigned_to is user:\n",
    "            return\n",
    "        if task.assigned_to is not None:\n",
    "            # remove from previous owner if any (reassignment)\n",
    "            prev = task.assigned_to\n",
    "            self.managers[prev.name].release_task(task)\n",
    "            task.reassignments += 1\n",
    "            if PRINT_EVENTS:\n",
    "                print(f\"â†ª Reassigned Task#{task.id} from {prev.name} to {user.name}\")\n",
    "\n",
    "        self.managers[user.name].accept_task(task)\n",
    "        task.assigned_to = user\n",
    "        task.status = \"assigned\"\n",
    "        if PRINT_EVENTS:\n",
    "            eff = user.effective_load(task)\n",
    "            print(f\"âœ“ Assigned Task#{task.id} ({task.task_type}, L={task.intrinsic_load:.2f}, eff={eff:.2f}) \"\n",
    "                  f\"to {user.name}. Load={user.current_load:.2f}\")\n",
    "\n",
    "    # ---------------------------- Monitoring ---------------------------------\n",
    "\n",
    "    def overloaded_users(self) -> List[User]:\n",
    "        return [u for u in self.users if u.current_load > MAX_LOAD]\n",
    "\n",
    "    def high_switching_users(self, threshold_unique_types: int = 3) -> List[User]:\n",
    "        \"\"\"Proxy: If last few tasks include many distinct types, flag frequent switching.\"\"\"\n",
    "        flagged = []\n",
    "        for u in self.users:\n",
    "            if len(set(u.recent_types)) >= threshold_unique_types and len(u.recent_types) >= 4:\n",
    "                flagged.append(u)\n",
    "        return flagged\n",
    "\n",
    "    # ----------------------------- Rebalancing --------------------------------\n",
    "\n",
    "    def monitor_and_rebalance(self, tasks: List[Task]):\n",
    "        # 1) Handle hard overloads\n",
    "        for u in self.overloaded_users():\n",
    "            if PRINT_EVENTS:\n",
    "                print(f\"âš  {u.name} OVERLOADED (Load={u.current_load:.2f}). Attempting to offload a task...\")\n",
    "            # Move the most recent or heaviest task first\n",
    "            if not u.active_tasks:\n",
    "                continue\n",
    "            task_to_move = max(u.active_tasks, key=lambda t: u.effective_load(t))\n",
    "            new_owner = self.find_best_user_for(task_to_move)\n",
    "            if new_owner and new_owner is not u:\n",
    "                self.assign_task(task_to_move, new_owner)\n",
    "\n",
    "        # 2) Handle high task switching (reduce extraneous load)\n",
    "        for u in self.high_switching_users():\n",
    "            if PRINT_EVENTS:\n",
    "                print(f\"âœ± {u.name} shows high task switching ({u.recent_types}). \"\n",
    "                      f\"Trying to consolidate similar tasks elsewhere.\")\n",
    "            # Pick one task type to keep; move the others if possible\n",
    "            if not u.active_tasks:\n",
    "                continue\n",
    "            keep_type = u.active_tasks[-1].task_type\n",
    "            for t in list(u.active_tasks):\n",
    "                if t.task_type != keep_type:\n",
    "                    candidate = self.find_best_user_for(t)\n",
    "                    if candidate and candidate is not u:\n",
    "                        self.assign_task(t, candidate)\n",
    "\n",
    "    # ----------------------------- Completion ---------------------------------\n",
    "\n",
    "    def complete_some_tasks(self) -> Dict[str, List[Task]]:\n",
    "        \"\"\"Each user completes up to N tasks, with latency & error proxies.\n",
    "        Returns dict of completed tasks per user.\"\"\"\n",
    "        completed: Dict[str, List[Task]] = defaultdict(list)\n",
    "        for u in self.users:\n",
    "            # choose tasks to complete: favor earliest assigned (FIFO)\n",
    "            to_complete = u.active_tasks[:COMPLETE_PER_USER_PER_ROUND]\n",
    "            for t in to_complete:\n",
    "                before_load = u.current_load\n",
    "                # latency proxy ~ effective load * base; add small randomness\n",
    "                eff = u.effective_load(t)\n",
    "                latency = eff * BASE_TIME_PER_LOAD * random.uniform(0.8, 1.2)\n",
    "\n",
    "                # error probability depends on current load\n",
    "                error_prob = next((p for th, p in ERROR_PROB_BANDS if before_load > th), 0.02)\n",
    "                did_error = random.random() < error_prob\n",
    "\n",
    "                # Complete task: reduce load, mark done\n",
    "                self.managers[u.name].release_task(t)\n",
    "                t.assigned_to = None\n",
    "                t.status = \"completed\"\n",
    "                completed[u.name].append(t)\n",
    "\n",
    "                if PRINT_EVENTS:\n",
    "                    outcome = \"ERROR\" if did_error else \"SUCCESS\"\n",
    "                    print(f\"ðŸ {u.name} completed Task#{t.id} [{t.task_type}] \"\n",
    "                          f\"in {latency:.1f}s with {outcome}. New Load={u.current_load:.2f}\")\n",
    "        return completed\n",
    "\n",
    "# ------------------------------- Simulation ----------------------------------\n",
    "\n",
    "class Simulation:\n",
    "    def __init__(self, users: List[User], tasks: List[Task]):\n",
    "        self.users = users\n",
    "        self.tasks = tasks\n",
    "        self.coord = CoordinatorAI(users)\n",
    "\n",
    "    def run(self, assign_batch: int = ASSIGN_BATCH_SIZE):\n",
    "        # Assign in batches to let load accumulate, then rebalance and complete\n",
    "        pending = [t for t in self.tasks if t.status == \"pending\"]\n",
    "        batch = []\n",
    "        while pending or any(u.active_tasks for u in self.users):\n",
    "            # Fill a batch\n",
    "            while pending and len(batch) < assign_batch:\n",
    "                batch.append(pending.pop(0))\n",
    "                time.sleep(0.8)   # pause for half a second To be able to see \n",
    "            # Assign current batch\n",
    "            for t in batch:\n",
    "                best = self.coord.find_best_user_for(t)\n",
    "                if best is None:\n",
    "                    # If no one has capacity, assign to the least-loaded anyway (soft overflow)\n",
    "                    best = min(self.users, key=lambda u: u.current_load)\n",
    "                    if PRINT_EVENTS:\n",
    "                        print(f\"â€¦ No capacity. Forcing assignment of Task#{t.id} to least-loaded {best.name}.\")\n",
    "                self.coord.assign_task(t, best)\n",
    "\n",
    "            # Monitor & rebalance after the batch\n",
    "            self.coord.monitor_and_rebalance(self.tasks)\n",
    "\n",
    "            # Complete some tasks across users, then monitor again\n",
    "            self.coord.complete_some_tasks()\n",
    "            self.coord.monitor_and_rebalance(self.tasks)\n",
    "\n",
    "            # Reset batch and refresh pending list\n",
    "            batch = []\n",
    "            pending = [t for t in self.tasks if t.status == \"pending\"]\n",
    "\n",
    "        if PRINT_EVENTS:\n",
    "            print(\"\\nâœ… Simulation finished. All tasks completed.\\n\")\n",
    "            self.report_summary()\n",
    "\n",
    "    def report_summary(self):\n",
    "        reassign_total = sum(t.reassignments for t in self.tasks)\n",
    "        print(f\"Tasks completed: {len([t for t in self.tasks if t.status=='completed'])}/{len(self.tasks)}\")\n",
    "        print(f\"Total reassignments (load balancing actions): {reassign_total}\")\n",
    "        # Simple distribution report by type and by user\n",
    "        by_type = defaultdict(int)\n",
    "        for t in self.tasks:\n",
    "            by_type[t.task_type] += 1\n",
    "        print(\"Tasks by type:\", dict(by_type))\n",
    "        # Note: if you want per-user completions, track in complete_some_tasks and aggregate here.\n",
    "\n",
    "# ------------------------------- Utilities -----------------------------------\n",
    "\n",
    "def make_users() -> List[User]:\n",
    "    # Example team with balanced BDI profiles\n",
    "    return [\n",
    "        User(\"Alice\", style='A'),   # Analytical\n",
    "        User(\"Bob\",   style='B'),   # Sequential\n",
    "        User(\"Cara\",  style='C'),   # Interpersonal\n",
    "        User(\"Dan\",   style='D'),   # imaginative\n",
    "    ]\n",
    "\n",
    "def make_tasks(n: int) -> List[Task]:\n",
    "    rng = random.Random(RANDOM_SEED + 1)\n",
    "    counter = itertools.count()\n",
    "    tasks = []\n",
    "    for _ in range(n):\n",
    "        ttype = rng.choice(TASK_TYPES)\n",
    "        intrinsic = rng.uniform(1.0, 5.0)  # intrinsic complexity between 1 and 5\n",
    "        tasks.append(Task(id=next(counter), task_type=ttype, intrinsic_load=intrinsic))\n",
    "    return tasks\n",
    "\n",
    "# --------------------------------- Main --------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(RANDOM_SEED)\n",
    "\n",
    "    users = make_users()\n",
    "    tasks = make_tasks(TASK_COUNT)\n",
    "\n",
    "    print(\"=== Cognitive Load Balancing Simulation ===\")\n",
    "    print(f\"Users: {[f'{u.name}:{u.style}' for u in users]}\")\n",
    "    print(f\"Generating {len(tasks)} tasksâ€¦\\n\")\n",
    "\n",
    "    sim = Simulation(users, tasks)\n",
    "    sim.run(assign_batch=ASSIGN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aebc654-b4b5-4a45-919d-76efc92adcb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb69475-2e08-485b-b553-7754eb7d856b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
